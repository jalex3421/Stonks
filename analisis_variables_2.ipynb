{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import neighbors, model_selection, tree\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('all_stocks_5yr.csv', delimiter=',')\n",
    "#se eliminan las filas que tienen valores nulos de cara a realizar posteriores estudios\n",
    "#HABRIA QUE TENERLO IMPUTADO CON VALORES PERDIDOS!!\n",
    "dataset.dropna(axis=0, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "atSalida = 'close'\n",
    "atEntrada = ['open','high','low','volume']\n",
    "\n",
    "dataset_output = dataset[atSalida].copy()\n",
    "dataset.drop(atSalida, axis=1, inplace=True)\n",
    "dataset = dataset[atEntrada].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto , las columnas nulas son open, high y  low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Transformación de variables categóricas a numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se va a realizar dicho tipo de transformación , pues para el proyecto se realizará principalmente redes neuronales y regresión polinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Reducción de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGREGAR DESPUES VARIABLE FECHA!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtención del rendimiento con todas las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar la calidad de las técnicas de selección de variables, se va a calcular el rendimiento de una regresión polinomial  si utilizamos todas las variables. Nos hemos decidido por esta técnica, pues es fácil de implementar.\n",
    "\n",
    "Se aplicará el método hold-out para obtener los conjuntos de entrenamiento y de test (70% de datos para entrenar).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE ELIMINARAN LAS FILAS ASOCIADAS A LOS VALORES NULOS PARA PODER REALIZAR LAS COSAS!!! JUNTAR CON LO DE IVAN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendimiento con regresión polinomial sobre todas las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, dataset_output, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error en entrenamiento es 0.4663 y en test es 0.4903\n"
     ]
    }
   ],
   "source": [
    "def regresion_polinomial(dataset,dataset_output, p_degree):\n",
    "    # Generamos los conjuntos de entrenamiento y de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset, dataset_output, test_size=0.2, random_state=12)\n",
    "    # Creamos una Pipeline en la que generamos variables polinómicas de grado 2, estandarizamos los datos y aprendemos una regresión lineal\n",
    "    pipePolinomial = Pipeline([('polynomial', PolynomialFeatures(degree=p_degree)),('scaler', StandardScaler()),('regressor', LinearRegression())])\n",
    "    # Entrenamos la Pipeline\n",
    "    pipePolinomial.fit(X_train, y_train)\n",
    "    # Obtenemos el rendimiento en entrenamiento y en test (MSE)\n",
    "    prTrain = pipePolinomial.predict(X_train)\n",
    "    errorTrain = mean_squared_error(y_train, prTrain)\n",
    "    prTest = pipePolinomial.predict(X_test)\n",
    "    errorTest = mean_squared_error(y_test, prTest)\n",
    "    print('El error en entrenamiento es {:.4f} y en test es {:.4f}'.format(errorTrain, errorTest))\n",
    "regresion_polinomial(dataset,dataset_output,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "correlaciones = X_train.corr(method ='pearson')\n",
    "correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la matriz de correlaciones especificando el rango de los valores [-1, 1]\n",
    "fig = plt.figure(figsize=(50,50))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlaciones, vmin=-1, vmax=1, cmap=plt.cm.rainbow)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(X_train.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "# Añadimos los nombres de las variables en la figura\n",
    "names = X_train.columns\n",
    "ax.set_xticklabels(names, rotation='90')\n",
    "ax.set_yticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras la obtención de matriz de correlaciones, se puede determinar que las variables de entrada están relacionadas entre si de manera muy alta, salvo el volumen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora el rendimiento que se obtendría al eliminar una de éstas variables. Para hacer más completo el experimento, se procederá a crear 3 datasets distintos: cada uno eliminando una variable distinta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Eliminación* *de* *open*, *high*, *low* por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset.copy()\n",
    "dataset_output1 = dataset_output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['open', 'high', 'low']   \n",
    "for nombrevar in variables:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset1, dataset_output1, test_size=0.2, random_state=12)\n",
    "    X_train = X_train.drop([nombrevar],axis=1)\n",
    "    X_test = X_test.drop([nombrevar],axis=1)\n",
    "\n",
    "    pipePolinomial = Pipeline([('polynomial', PolynomialFeatures(degree=2)),('scaler', StandardScaler()),('regressor', LinearRegression())])\n",
    "    pipePolinomial.fit(X_train, y_train)\n",
    "\n",
    "    #Se obtiene el rendimiento en entrenamiento y en test (MSE)\n",
    "    prTrain = pipePolinomial.predict(X_train)\n",
    "    errorTrain = mean_squared_error(y_train, prTrain)\n",
    "    prTest = pipePolinomial.predict(X_test)\n",
    "    errorTest = mean_squared_error(y_test, prTest)\n",
    "    print(f\"Resutados para la eliminacion de {nombrevar}\")\n",
    "    print('El error en entrenamiento es {:.4f} y en test es {:.4f}'.format(errorTrain, errorTest))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Eliminación* *por* *pares*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pares = [ ['open', 'high'] , ['open', 'low'] , ['high', 'low']]   \n",
    "for par in pares:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset1, dataset_output1, test_size=0.2, random_state=12)\n",
    "    \n",
    "    X_train = X_train.drop([par[0]],axis=1)\n",
    "    X_train = X_train.drop([par[1]],axis=1)\n",
    "\n",
    "    X_test = X_test.drop([par[0]],axis=1)\n",
    "    X_test = X_test.drop([par[1]],axis=1)\n",
    "\n",
    "\n",
    "    pipePolinomial = Pipeline([('polynomial', PolynomialFeatures(degree=2)),('scaler', StandardScaler()),('regressor', LinearRegression())])\n",
    "    pipePolinomial.fit(X_train, y_train)\n",
    "\n",
    "    #Se obtiene el rendimiento en entrenamiento y en test (MSE)\n",
    "    prTrain = pipePolinomial.predict(X_train)\n",
    "    errorTrain = mean_squared_error(y_train, prTrain)\n",
    "    prTest = pipePolinomial.predict(X_test)\n",
    "    errorTest = mean_squared_error(y_test, prTest)\n",
    "    print(f\"Resutados para la eliminacion de {par[0]} y {par[1]}\")\n",
    "    print('El error en entrenamiento es {:.4f} y en test es {:.4f}'.format(errorTrain, errorTest))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Eliminación* *de* *las* *tres* *variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset1, dataset_output1, test_size=0.2, random_state=12)\n",
    "    \n",
    "X_train = X_train.drop(['open'] ,axis=1)\n",
    "X_train = X_train.drop(['low'],axis=1)\n",
    "X_train = X_train.drop(['high'] ,axis=1)\n",
    "\n",
    "X_test = X_test.drop(['open'] ,axis=1)\n",
    "X_test = X_test.drop(['low'],axis=1)\n",
    "X_test = X_test.drop(['high'] ,axis=1)\n",
    "\n",
    "pipePolinomial = Pipeline([('polynomial', PolynomialFeatures(degree=2)),('scaler', StandardScaler()),('regressor', LinearRegression())])\n",
    "pipePolinomial.fit(X_train, y_train)\n",
    "\n",
    "#Se obtiene el rendimiento en entrenamiento y en test (MSE)\n",
    "prTrain = pipePolinomial.predict(X_train)\n",
    "errorTrain = mean_squared_error(y_train, prTrain)\n",
    "prTest = pipePolinomial.predict(X_test)\n",
    "errorTest = mean_squared_error(y_test, prTest)\n",
    "print(\"Resultado para la eliminacion de high, low, y open\")\n",
    "print('El error en entrenamiento es {:.4f} y en test es {:.4f}'.format(errorTrain, errorTest))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras los experimentos realizados, se puede observar que se obtiene un peor resultado al tratar de eliminar cualquiera de las tres variables que cuentan con una mayor correlación -casi perfecta- .\n",
    "\n",
    "Por otro lado, en caso de eliminar una variable,la que menos impacto tiene en el rendimiento del modelo es 'open', y la que más influye es 'high'. \n",
    "\n",
    "En caso de eliminar dos variables,el mayor impacto negativo en el rendimiento vendrá dado por la eliminación de conjunta de 'low' y 'high'. Cosa que es natural, pues representa el valor más bajo y más alto de las acciones de una empresa en un día.\n",
    "\n",
    "La eliminación de 'high', 'low' y 'open' provoca un desenlace nefasto y esperado, pues predecir el valor al final del día de las acciones a partir del volumen de acciones negociadas es sumamente complicado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparativas de métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La característica principal de las redes recurrentes es que la información puede persistir introduciendo bucles en el diagrama de la red, por lo que, básicamente, pueden «recordar» estados previos y utilizar esta información para decidir cuál será el siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera sencilla, pueden ser pensadas como múltiples copias de la misma red, cada una pasando un mensaje a su sucesor. Por lo que, gracias a su naturaleza, pueden ser relacionadas con listas o secuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las LSTM pueden aprender dependencias largas, por lo que se podría decir que tienen una «memoria» a más largo plazo. Lo que les hace atractivas para este tipo de problemas, donde la información pasada puede jugar un papel importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIBUJO!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/44/e1/dc0757b20b56c980b5553c1b5c4c32d378c7055ab7bfa92006801ad359ab/Keras-2.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from keras) (1.3.1)\n",
      "Collecting pyyaml (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/66/7a/9f3a49dea61e98ac8bd3b6b619da4fcb59053232a6e11e9e865dab7c16e0/PyYAML-5.4.1-cp37-cp37m-win_amd64.whl (210kB)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/5e/cc78e74c89863a8e6269cfde9b8588dca079e29894e12df7036b9acfea09/h5py-3.2.1-cp37-cp37m-win_amd64.whl (2.7MB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from keras) (1.16.4)\n",
      "Collecting cached-property; python_version < \"3.8\" (from h5py->keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
      "Installing collected packages: pyyaml, cached-property, h5py, keras\n",
      "Successfully installed cached-property-1.5.2 h5py-3.2.1 keras-2.4.3 pyyaml-5.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/68/8c/42bbb31a25a708e2e24881724ec7bcea05530492de8b1a2e0d8fe43eb2f6/tensorflow-2.4.1-cp37-cp37m-win_amd64.whl (370.7MB)\n",
      "Collecting astunparse~=1.6.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Collecting google-pasta~=0.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
      "Collecting termcolor~=1.1.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting absl-py~=0.10 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/92/c9/ef0fae29182d7a867d203f0eff8296b60da92098cc41db33a434f4be84bf/absl_py-0.12.0-py3-none-any.whl (129kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n",
      "Collecting typing-extensions~=3.7.4 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Collecting protobuf>=3.9.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/9c/2609ddd0774f721a028352fcf864b9e4ec762099cdce0ea75462e00a7f57/protobuf-3.16.0-cp37-cp37m-win_amd64.whl (904kB)\n",
      "Collecting numpy~=1.19.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/18/60ac053857fb924b0324c81200b59c00317ebaa3c14b7478266b50ffed19/numpy-1.19.5-cp37-cp37m-win_amd64.whl (13.2MB)\n",
      "Collecting h5py~=2.10.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/6b/7f62017e3f0b32438dd90bdc1ff0b7b1448b6cb04a1ed84f37b6de95cd7b/h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5MB)\n",
      "Collecting opt-einsum~=3.3.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
      "Collecting wrapt~=1.12.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
      "Collecting flatbuffers~=1.12.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
      "Collecting tensorboard~=2.4 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0MB)\n",
      "Collecting keras-preprocessing~=1.1.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "Collecting grpcio~=1.32.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/67/5f/bf822211f7f94a2f6d0f8fd3bda3b804d7b24b6d5c84dbc6e6c9df4c74c2/grpcio-1.32.0-cp37-cp37m-win_amd64.whl (2.5MB)\n",
      "Collecting wheel~=0.35 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n",
      "Collecting six~=1.15.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting gast==0.3.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/69/5747a957f95e2e1d252ca41476ae40ce79d70d38151d2e494feb7722860c/tensorboard_data_server-0.6.1-py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/c1/44179a1cfc5c3b5832a5f9c925161612471ec5f346bcd186235651d74f35/google_auth-1.30.0-py2.py3-none-any.whl (146kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (41.0.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/c1/499e600ba0c618b451cd9c425ae1c177249940a2086316552fee7d86c954/tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/9d/d3/7541e89f1fc456eef157224f597a8bba22589db6369a03eaba68c11f07a0/google_auth_oauthlib-0.4.4-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/33/1ae0f71395e618d6140fbbc9587cc3156591f748226075e0f7d6f9176522/Markdown-3.3.4-py3-none-any.whl (97kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\" (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/93/0c0f002031f18b53af7a6166103c02b9c0667be528944137cc954ec921b3/rsa-4.7.2-py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/28/c4f5796c67ad06bb91d98d543a5e01805c1ff065e08871f78e52d2a331ad/cachetools-4.2.2-py3-none-any.whl\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl (153kB)\n",
      "Collecting chardet<5,>=3.0.2 (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl (178kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata; python_version < \"3.8\" (from markdown>=2.6.8->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/e2/49966924c93909d47612bb47d911448140a2f6c1390aec2f4c1afbe3748f/importlib_metadata-4.0.1-py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/8c/715c54e9e34c0c4820f616a913a7de3337d0cd79074dd1bed4dd840f16ae/zipp-3.4.1-py3-none-any.whl\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4835 sha256=c8d498282aa7df43ad37b03b85e2018b44b1db41bcaeaa7eb8bd5f84652a9095\n",
      "  Stored in directory: C:\\Users\\alumno\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-none-any.whl size=19559 sha256=7b0da023d92a3fa10c600d0a3437efa058d9249bbf7a7d996f6149395bb05327\n",
      "  Stored in directory: C:\\Users\\alumno\\AppData\\Local\\pip\\Cache\\wheels\\b1\\c2\\ed\\d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: wheel, six, astunparse, google-pasta, termcolor, absl-py, tensorflow-estimator, typing-extensions, protobuf, numpy, h5py, opt-einsum, wrapt, flatbuffers, tensorboard-data-server, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, tensorboard-plugin-wit, urllib3, chardet, idna, requests, werkzeug, oauthlib, requests-oauthlib, google-auth-oauthlib, zipp, importlib-metadata, markdown, grpcio, tensorboard, keras-preprocessing, gast, tensorflow\n",
      "  Found existing installation: wheel 0.33.4\n",
      "    Uninstalling wheel-0.33.4:\n",
      "      Successfully uninstalled wheel-0.33.4\n",
      "  Found existing installation: six 1.12.0\n",
      "    Uninstalling six-1.12.0:\n",
      "      Successfully uninstalled six-1.12.0\n",
      "  Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Acceso denegado: 'c:\\\\programdata\\\\anaconda3\\\\envs\\\\py37machlearn\\\\lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37machlearn\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2faa7ec4c8d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#importing the packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37machlearn\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     raise ImportError(\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "#importing the packages \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de datos\n",
    "todataframe = pd.read_csv('all_stocks_5yr.csv', delimiter=',')\n",
    "todataframe.dropna(axis=0, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La comparativa del método se hará con la empresa AAl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = todataframe[todataframe['Name']=='AAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear un conjunto de entrenamiento y de test (70% para train, 30% para test)\n",
    "numero_ejemplos_entrenamiento = int(len(df)*0.7)\n",
    "training_set = df.iloc[:numero_ejemplos_entrenamiento, 1:2].values\n",
    "test_set = df.iloc[numero_ejemplos_entrenamiento:, 1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)# Creating a data structure with 60 time-steps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 800):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-d96186e763aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#Adding the first LSTM layer and some Dropout regularisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Adding a second LSTM layer and some Dropout regularisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Adding a third LSTM layer and some Dropout regularisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()#Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))# Adding a second LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))# Adding a third LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "dataset_train = df.iloc[:numero_ejemplos_entrenamiento, 1:2]\n",
    "dataset_test = df.iloc[numero_ejemplos_entrenamiento:, 1:2]dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].valuesinputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "for i in range(60, 519):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion del dataframe\n",
    "seriesdata = todataframe.sort_index(ascending=True, axis=0)\n",
    "new_seriesdata = pd.DataFrame(index=range(0,len(todataframe)),columns=['date','close'])\n",
    "length_of_data=len(seriesdata)\n",
    "for i in range(0,length_of_data):\n",
    "    new_seriesdata['date'][i] = seriesdata['date'][i]\n",
    "    new_seriesdata['close'][i] = seriesdata['close'][i]\n",
    "\n",
    "    \n",
    "#Colocar los indices de nuevo\n",
    "new_seriesdata.index = new_seriesdata.date\n",
    "new_seriesdata.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear un conjunto de entrenamiento y de test (70% para train, 30% para test)\n",
    "numero_ejemplos_entrenamiento = int(len(new_seriesdata)*0.7)\n",
    "\n",
    "#Crear conjuntos de train y test con el total de los elementos\n",
    "myseriesdataset = new_seriesdata.values\n",
    "totrain = myseriesdataset[0:numero_ejemplos_entrenamiento,:] #conjunto de ejemplos para entrenar\n",
    "tovalid = myseriesdataset[numero_ejemplos_entrenamiento:,:] #conjunto de ejemplos para validar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "821/821 - 25s - loss: 0.2391 - mean_squared_error: 0.2391\n",
      "Epoch 2/3\n",
      "821/821 - 22s - loss: 0.1745 - mean_squared_error: 0.1745\n",
      "Epoch 3/3\n",
      "821/821 - 23s - loss: 0.1148 - mean_squared_error: 0.1148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24cb8f8acc8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convertir el dataset en x_train e y_train\n",
    "scalerdata = MinMaxScaler(feature_range=(0, 1))\n",
    "scale_data = scalerdata.fit_transform(myseriesdataset)\n",
    "x_totrain, y_totrain = [], []\n",
    "length_of_totrain=len(totrain)\n",
    "for i in range(60,length_of_totrain):\n",
    "    x_totrain.append(scale_data[i-60:i,0])\n",
    "    y_totrain.append(scale_data[i,0])\n",
    "x_totrain, y_totrain = np.array(x_totrain), np.array(y_totrain)\n",
    "x_totrain = np.reshape(x_totrain, (x_totrain.shape[0],x_totrain.shape[1],1))\n",
    "\n",
    "#LSTM neural network configuracion\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=50, return_sequences=True, input_shape=(x_totrain.shape[1],1)))\n",
    "lstm_model.add(LSTM(units=50))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='adadelta', metrics= ['mean_squared_error'])\n",
    "lstm_model.fit(x_totrain, y_totrain, epochs=3, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecir\n",
    "myinputs = new_seriesdata[len(new_seriesdata) - (len(tovalid)) - 60:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "myinputs  = scalerdata.transform(myinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2491232 ],\n",
       "       [0.24908283],\n",
       "       [0.24899095],\n",
       "       [0.24907307],\n",
       "       [0.24926008],\n",
       "       [0.24975638],\n",
       "       [0.25067288],\n",
       "       [0.25178325],\n",
       "       [0.2529819 ],\n",
       "       [0.2541405 ],\n",
       "       [0.25509575],\n",
       "       [0.255988  ],\n",
       "       [0.25688314],\n",
       "       [0.25776976],\n",
       "       [0.2584892 ],\n",
       "       [0.25905514],\n",
       "       [0.25974008],\n",
       "       [0.26029006],\n",
       "       [0.26070642],\n",
       "       [0.26110286],\n",
       "       [0.26159295],\n",
       "       [0.2626034 ],\n",
       "       [0.26408416],\n",
       "       [0.26557723],\n",
       "       [0.26696166],\n",
       "       [0.26796892],\n",
       "       [0.26843005],\n",
       "       [0.26852897],\n",
       "       [0.2681647 ],\n",
       "       [0.2675032 ],\n",
       "       [0.26661348],\n",
       "       [0.26548728],\n",
       "       [0.2644061 ],\n",
       "       [0.2634856 ],\n",
       "       [0.2625374 ],\n",
       "       [0.26188037],\n",
       "       [0.26140264],\n",
       "       [0.26104096],\n",
       "       [0.26097092],\n",
       "       [0.26138306],\n",
       "       [0.26207218],\n",
       "       [0.26296067],\n",
       "       [0.26401982],\n",
       "       [0.26503006],\n",
       "       [0.2662667 ],\n",
       "       [0.26743725],\n",
       "       [0.26829156],\n",
       "       [0.26939282],\n",
       "       [0.2704896 ],\n",
       "       [0.27150834],\n",
       "       [0.27264214],\n",
       "       [0.27415156],\n",
       "       [0.2758487 ],\n",
       "       [0.27741337],\n",
       "       [0.2788037 ],\n",
       "       [0.27999774],\n",
       "       [0.28092998],\n",
       "       [0.28181195],\n",
       "       [0.2826838 ],\n",
       "       [0.28365818],\n",
       "       [0.28446576],\n",
       "       [0.2850985 ],\n",
       "       [0.28551602],\n",
       "       [0.2858723 ],\n",
       "       [0.2866359 ],\n",
       "       [0.2877113 ],\n",
       "       [0.28911626],\n",
       "       [0.29065487],\n",
       "       [0.29236802],\n",
       "       [0.29425287],\n",
       "       [0.29651758],\n",
       "       [0.29888   ],\n",
       "       [0.30153757],\n",
       "       [0.30441153],\n",
       "       [0.30726752],\n",
       "       [0.30999228],\n",
       "       [0.31259242],\n",
       "       [0.31513837],\n",
       "       [0.31748793],\n",
       "       [0.31970948],\n",
       "       [0.32167155],\n",
       "       [0.32321993],\n",
       "       [0.32452282],\n",
       "       [0.32555   ],\n",
       "       [0.32652965],\n",
       "       [0.32799438],\n",
       "       [0.329621  ],\n",
       "       [0.3316293 ],\n",
       "       [0.33335996],\n",
       "       [0.33483952],\n",
       "       [0.3361389 ],\n",
       "       [0.33736843],\n",
       "       [0.33836296],\n",
       "       [0.33937514],\n",
       "       [0.34054008],\n",
       "       [0.34182745],\n",
       "       [0.34296435],\n",
       "       [0.343889  ],\n",
       "       [0.34467277],\n",
       "       [0.34512806],\n",
       "       [0.3452386 ],\n",
       "       [0.34499562],\n",
       "       [0.3444422 ],\n",
       "       [0.34380797],\n",
       "       [0.3429759 ],\n",
       "       [0.3421081 ],\n",
       "       [0.34147117],\n",
       "       [0.3413533 ],\n",
       "       [0.34163433],\n",
       "       [0.34201998],\n",
       "       [0.34232736],\n",
       "       [0.34234875],\n",
       "       [0.3423742 ],\n",
       "       [0.34234333],\n",
       "       [0.34243754],\n",
       "       [0.3423818 ],\n",
       "       [0.34234136],\n",
       "       [0.34241992],\n",
       "       [0.34296763],\n",
       "       [0.34321964],\n",
       "       [0.34270948],\n",
       "       [0.3415432 ],\n",
       "       [0.33996394],\n",
       "       [0.33818582],\n",
       "       [0.33640498],\n",
       "       [0.334965  ],\n",
       "       [0.33379117],\n",
       "       [0.33279252],\n",
       "       [0.3322308 ],\n",
       "       [0.33202723],\n",
       "       [0.33227503],\n",
       "       [0.3326138 ],\n",
       "       [0.3331913 ],\n",
       "       [0.33378512],\n",
       "       [0.33432868],\n",
       "       [0.33479437],\n",
       "       [0.33507484],\n",
       "       [0.33516482],\n",
       "       [0.33517388],\n",
       "       [0.3351503 ],\n",
       "       [0.33512005],\n",
       "       [0.33526194],\n",
       "       [0.33520195],\n",
       "       [0.33523843],\n",
       "       [0.33501172],\n",
       "       [0.3344726 ],\n",
       "       [0.33376798],\n",
       "       [0.33261976],\n",
       "       [0.33134073],\n",
       "       [0.32968372],\n",
       "       [0.32753482],\n",
       "       [0.32535893],\n",
       "       [0.32329017],\n",
       "       [0.3212661 ],\n",
       "       [0.31933165],\n",
       "       [0.31720865],\n",
       "       [0.31500092],\n",
       "       [0.31309086],\n",
       "       [0.3115414 ],\n",
       "       [0.31027856],\n",
       "       [0.30944982],\n",
       "       [0.30877712],\n",
       "       [0.30834818],\n",
       "       [0.30803746],\n",
       "       [0.30783865],\n",
       "       [0.307331  ],\n",
       "       [0.30671576],\n",
       "       [0.3061748 ],\n",
       "       [0.30573314],\n",
       "       [0.3054987 ],\n",
       "       [0.30581295],\n",
       "       [0.30649608],\n",
       "       [0.30722138],\n",
       "       [0.30814323],\n",
       "       [0.30906788],\n",
       "       [0.31008956],\n",
       "       [0.31134698],\n",
       "       [0.3127021 ],\n",
       "       [0.314372  ],\n",
       "       [0.31624576],\n",
       "       [0.31811967],\n",
       "       [0.31932727],\n",
       "       [0.31967992],\n",
       "       [0.3193354 ],\n",
       "       [0.31901228],\n",
       "       [0.31878278],\n",
       "       [0.31861866],\n",
       "       [0.31864873],\n",
       "       [0.31891742],\n",
       "       [0.3198486 ],\n",
       "       [0.3212562 ],\n",
       "       [0.3227146 ],\n",
       "       [0.32394838],\n",
       "       [0.32517856],\n",
       "       [0.3264456 ],\n",
       "       [0.327208  ],\n",
       "       [0.32750726],\n",
       "       [0.3276488 ],\n",
       "       [0.3279467 ],\n",
       "       [0.32846573],\n",
       "       [0.3291374 ],\n",
       "       [0.33017328],\n",
       "       [0.33159617],\n",
       "       [0.33303097],\n",
       "       [0.33449307],\n",
       "       [0.33607674],\n",
       "       [0.3377978 ],\n",
       "       [0.339597  ],\n",
       "       [0.34137562],\n",
       "       [0.34332594],\n",
       "       [0.3454683 ],\n",
       "       [0.34739244],\n",
       "       [0.3488845 ],\n",
       "       [0.35002407],\n",
       "       [0.3508936 ],\n",
       "       [0.35150227],\n",
       "       [0.351849  ],\n",
       "       [0.35221392],\n",
       "       [0.35223743],\n",
       "       [0.35208857],\n",
       "       [0.3519848 ],\n",
       "       [0.35185456],\n",
       "       [0.35173556],\n",
       "       [0.35157093],\n",
       "       [0.35154685],\n",
       "       [0.3517199 ],\n",
       "       [0.3521738 ],\n",
       "       [0.35281682],\n",
       "       [0.35374317],\n",
       "       [0.35500798],\n",
       "       [0.35666648],\n",
       "       [0.3584438 ],\n",
       "       [0.35996857],\n",
       "       [0.36171976],\n",
       "       [0.36362144],\n",
       "       [0.36561733],\n",
       "       [0.36752674],\n",
       "       [0.36913416],\n",
       "       [0.3703488 ],\n",
       "       [0.37120068],\n",
       "       [0.37169906],\n",
       "       [0.37181467],\n",
       "       [0.37152854],\n",
       "       [0.3710796 ],\n",
       "       [0.37034875],\n",
       "       [0.36955035],\n",
       "       [0.36875674],\n",
       "       [0.36813402],\n",
       "       [0.3675292 ],\n",
       "       [0.36695692],\n",
       "       [0.36648816],\n",
       "       [0.36606047],\n",
       "       [0.3655361 ],\n",
       "       [0.36481825],\n",
       "       [0.3638007 ],\n",
       "       [0.36256945],\n",
       "       [0.36135805],\n",
       "       [0.36015934],\n",
       "       [0.35898057],\n",
       "       [0.3573139 ],\n",
       "       [0.35522527],\n",
       "       [0.35299158],\n",
       "       [0.35069668],\n",
       "       [0.3481085 ],\n",
       "       [0.34515053],\n",
       "       [0.34261063],\n",
       "       [0.34042653],\n",
       "       [0.33843657],\n",
       "       [0.3365719 ],\n",
       "       [0.33494765],\n",
       "       [0.33367983],\n",
       "       [0.3323586 ],\n",
       "       [0.33110505],\n",
       "       [0.3298404 ],\n",
       "       [0.32854414],\n",
       "       [0.32781804],\n",
       "       [0.32763422],\n",
       "       [0.3279618 ],\n",
       "       [0.3284115 ],\n",
       "       [0.32884216],\n",
       "       [0.32907254],\n",
       "       [0.3289278 ],\n",
       "       [0.32878438],\n",
       "       [0.32889017],\n",
       "       [0.32935122],\n",
       "       [0.33018294],\n",
       "       [0.33119494],\n",
       "       [0.332097  ],\n",
       "       [0.3330232 ],\n",
       "       [0.33394843],\n",
       "       [0.33489066],\n",
       "       [0.33646187],\n",
       "       [0.33848986],\n",
       "       [0.3406714 ],\n",
       "       [0.34301797],\n",
       "       [0.3452334 ],\n",
       "       [0.3478001 ],\n",
       "       [0.35051596],\n",
       "       [0.3531556 ],\n",
       "       [0.35564506],\n",
       "       [0.35784876],\n",
       "       [0.3597301 ],\n",
       "       [0.3613267 ],\n",
       "       [0.36257276],\n",
       "       [0.36363316],\n",
       "       [0.364349  ],\n",
       "       [0.36500835],\n",
       "       [0.36544374],\n",
       "       [0.36515585],\n",
       "       [0.36412126],\n",
       "       [0.36257875],\n",
       "       [0.3606894 ],\n",
       "       [0.3588899 ],\n",
       "       [0.3571469 ],\n",
       "       [0.35551175],\n",
       "       [0.35402566],\n",
       "       [0.35244748],\n",
       "       [0.3508189 ],\n",
       "       [0.34908256],\n",
       "       [0.34734166],\n",
       "       [0.34564975],\n",
       "       [0.34406695],\n",
       "       [0.34293863],\n",
       "       [0.342287  ],\n",
       "       [0.34189227],\n",
       "       [0.34172392],\n",
       "       [0.34193045],\n",
       "       [0.3423852 ],\n",
       "       [0.34294376],\n",
       "       [0.34350404],\n",
       "       [0.34422126],\n",
       "       [0.3450376 ],\n",
       "       [0.34616303],\n",
       "       [0.34713086],\n",
       "       [0.34812173],\n",
       "       [0.34901643],\n",
       "       [0.34983125],\n",
       "       [0.35085985],\n",
       "       [0.35205278],\n",
       "       [0.35336122],\n",
       "       [0.35450956],\n",
       "       [0.35539836],\n",
       "       [0.35610965],\n",
       "       [0.3568822 ],\n",
       "       [0.35778818],\n",
       "       [0.35874104],\n",
       "       [0.35978767],\n",
       "       [0.36105847],\n",
       "       [0.3623985 ],\n",
       "       [0.3637718 ],\n",
       "       [0.36501974],\n",
       "       [0.36613455],\n",
       "       [0.36703345],\n",
       "       [0.36795893],\n",
       "       [0.36875978],\n",
       "       [0.369498  ],\n",
       "       [0.37017876],\n",
       "       [0.37068367],\n",
       "       [0.37103182],\n",
       "       [0.37164092],\n",
       "       [0.37301403],\n",
       "       [0.37531805],\n",
       "       [0.3780552 ],\n",
       "       [0.38094223],\n",
       "       [0.38383734],\n",
       "       [0.38656044],\n",
       "       [0.3890564 ],\n",
       "       [0.3913505 ],\n",
       "       [0.39272544],\n",
       "       [0.39297557],\n",
       "       [0.39248312],\n",
       "       [0.39149627],\n",
       "       [0.39022505],\n",
       "       [0.38920537],\n",
       "       [0.38836434],\n",
       "       [0.38726968],\n",
       "       [0.38549557],\n",
       "       [0.38360286]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tostore_test_result = []\n",
    "for i in range(60,myinputs.shape[0]):\n",
    "    tostore_test_result.append(myinputs[i-60:i,0])\n",
    "tostore_test_result = np.array(tostore_test_result)\n",
    "tostore_test_result = np.reshape(tostore_test_result,(tostore_test_result.shape[0],tostore_test_result.shape[1],1))\n",
    "myclosing_priceresult = lstm_model.predict(tostore_test_result)\n",
    "myclosing_priceresult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_resultante = scalerdata.inverse_transform(myclosing_priceresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediccion_resultante.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tovalid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado del rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El MSE obtenido ha sido de : 345.25766857289824\n"
     ]
    }
   ],
   "source": [
    "print(f\"El MSE obtenido ha sido de : {mean_squared_error(tovalid, prediccion_resultante)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "todataframe = pd.read_csv('all_stocks_5yr.csv', delimiter=',')\n",
    "todataframe.dropna(axis=0, inplace = True) \n",
    "todataframe = todataframe[todataframe['Name']=='AAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>15.07</td>\n",
       "      <td>15.12</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.75</td>\n",
       "      <td>8407500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2013-02-11</td>\n",
       "      <td>14.89</td>\n",
       "      <td>15.01</td>\n",
       "      <td>14.26</td>\n",
       "      <td>14.46</td>\n",
       "      <td>8882000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2013-02-12</td>\n",
       "      <td>14.45</td>\n",
       "      <td>14.51</td>\n",
       "      <td>14.10</td>\n",
       "      <td>14.27</td>\n",
       "      <td>8126000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>14.30</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.25</td>\n",
       "      <td>14.66</td>\n",
       "      <td>10259500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.96</td>\n",
       "      <td>13.16</td>\n",
       "      <td>13.99</td>\n",
       "      <td>31879900</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   open   high    low  close    volume Name\n",
       "0  2013-02-08  15.07  15.12  14.63  14.75   8407500  AAL\n",
       "1  2013-02-11  14.89  15.01  14.26  14.46   8882000  AAL\n",
       "2  2013-02-12  14.45  14.51  14.10  14.27   8126000  AAL\n",
       "3  2013-02-13  14.30  14.94  14.25  14.66  10259500  AAL\n",
       "4  2013-02-14  14.94  14.96  13.16  13.99  31879900  AAL"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "atSalida = 'close'\n",
    "atEntrada = ['open','high','low','volume']\n",
    "\n",
    "todataframe_output = todataframe[atSalida].copy()\n",
    "todataframe.drop(atSalida, axis=1, inplace=True)\n",
    "todataframe = todataframe[atEntrada].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecucion apra regresion polinomial de grado 2\n",
      "El error en entrenamiento es 0.1138 y en test es 0.1101\n",
      "------------------------------------------------------\n",
      "Ejecucion apra regresion polinomial de grado 3\n",
      "El error en entrenamiento es 0.1036 y en test es 0.1452\n",
      "------------------------------------------------------\n",
      "Ejecucion apra regresion polinomial de grado 4\n",
      "El error en entrenamiento es 0.0945 y en test es 0.9647\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "grados= [2,3,4]\n",
    "\n",
    "for grado in grados:\n",
    "    print(f\"Ejecucion apra regresion polinomial de grado {grado}\")\n",
    "    regresion_polinomial(todataframe,todataframe_output,grado)\n",
    "    print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observaciones*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusiones** **sobre** **comparativa** **de** **métodos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar rendimiento con conjuntos de entrenamiento con regresion polinomial y LSTM "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

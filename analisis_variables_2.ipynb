{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import neighbors, model_selection, tree\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('all_stocks_5yr.csv', delimiter=',')\n",
    "#se eliminan las filas que tienen valores nulos de cara a realizar posteriores estudios\n",
    "#HABRIA QUE TENERLO IMPUTADO CON VALORES PERDIDOS!!\n",
    "dataset.dropna(axis=0, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "atSalida = 'close'\n",
    "atEntrada = ['open','high','low','volume']\n",
    "\n",
    "dataset_output = dataset[atSalida].copy()\n",
    "dataset.drop(atSalida, axis=1, inplace=True)\n",
    "dataset = dataset[atEntrada].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto , las columnas nulas son open, high y  low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Transformación de variables categóricas a numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se va a realizar dicho tipo de transformación , pues para el proyecto se realizará principalmente redes neuronales y regresión polinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Reducción de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGREGAR DESPUES VARIABLE FECHA!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtención del rendimiento con todas las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar la calidad de las técnicas de selección de variables, se va a calcular el rendimiento de una regresión polinomial  si utilizamos todas las variables. Nos hemos decidido por esta técnica, pues es fácil de implementar.\n",
    "\n",
    "Se aplicará el método hold-out para obtener los conjuntos de entrenamiento y de test (70% de datos para entrenar).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE ELIMINARAN LAS FILAS ASOCIADAS A LOS VALORES NULOS PARA PODER REALIZAR LAS COSAS!!! JUNTAR CON LO DE IVAN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendimiento con regresión polinomial sobre todas las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos los conjuntos de entrenamiento y de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, dataset_output, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error en entrenamiento es 0.4663 y en test es 0.4903\n"
     ]
    }
   ],
   "source": [
    "def regresion_polinomial():\n",
    "    # Creamos una Pipeline en la que generamos variables polinómicas de grado 2, estandarizamos los datos y aprendemos una regresión lineal\n",
    "    pipePolinomial = Pipeline([('polynomial', PolynomialFeatures(degree=2)),('scaler', StandardScaler()),('regressor', LinearRegression())])\n",
    "    # Entrenamos la Pipeline\n",
    "    pipePolinomial.fit(X_train, y_train)\n",
    "    # Obtenemos el rendimiento en entrenamiento y en test (MSE)\n",
    "    prTrain = pipePolinomial.predict(X_train)\n",
    "    errorTrain = mean_squared_error(y_train, prTrain)\n",
    "    prTest = pipePolinomial.predict(X_test)\n",
    "    errorTest = mean_squared_error(y_test, prTest)\n",
    "    print('El error en entrenamiento es {:.4f} y en test es {:.4f}'.format(errorTrain, errorTest))\n",
    "regresion_polinomial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "correlaciones = X_train.corr(method ='pearson')\n",
    "correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la matriz de correlaciones especificando el rango de los valores [-1, 1]\n",
    "fig = plt.figure(figsize=(50,50))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlaciones, vmin=-1, vmax=1, cmap=plt.cm.rainbow)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(X_train.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "# Añadimos los nombres de las variables en la figura\n",
    "names = X_train.columns\n",
    "ax.set_xticklabels(names, rotation='90')\n",
    "ax.set_yticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras la obtención de matriz de correlaciones, se puede determinar que las variables de entrada están relacionadas entre si de manera muy alta, salvo el volumen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora el rendimiento que se obtendría al eliminar una de éstas variables. Para hacer más completo el experimento, se procederá a crear 3 datasets distintos: cada uno eliminando una variable distinta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Eliminación* *de* *open*, *high*, *low* por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset.copy()\n",
    "dataset_output1 = dataset_output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['open', 'high', 'low']   \n",
    "for nombrevar in variables:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset1, dataset_output1, test_size=0.2, random_state=12)\n",
    "    X_train = X_train.drop([nombrevar],axis=1)\n",
    "    X_test = X_test.drop([nombrevar],axis=1)\n",
    "\n",
    "    pipePolinomial = Pipeline([('polynomial', PolynomialFeatures(degree=2)),('scaler', StandardScaler()),('regressor', LinearRegression())])\n",
    "    pipePolinomial.fit(X_train, y_train)\n",
    "\n",
    "    #Se obtiene el rendimiento en entrenamiento y en test (MSE)\n",
    "    prTrain = pipePolinomial.predict(X_train)\n",
    "    errorTrain = mean_squared_error(y_train, prTrain)\n",
    "    prTest = pipePolinomial.predict(X_test)\n",
    "    errorTest = mean_squared_error(y_test, prTest)\n",
    "    print(f\"Resutados para la eliminacion de {nombrevar}\")\n",
    "    print('El error en entrenamiento es {:.4f} y en test es {:.4f}'.format(errorTrain, errorTest))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Eliminación* *por* *pares*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pares = [ ['open', 'high'] , ['open', 'low'] , ['high', 'low']]   \n",
    "for par in pares:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset1, dataset_output1, test_size=0.2, random_state=12)\n",
    "    \n",
    "    X_train = X_train.drop([par[0]],axis=1)\n",
    "    X_train = X_train.drop([par[1]],axis=1)\n",
    "\n",
    "    X_test = X_test.drop([par[0]],axis=1)\n",
    "    X_test = X_test.drop([par[1]],axis=1)\n",
    "\n",
    "\n",
    "    pipePolinomial = Pipeline([('polynomial', PolynomialFeatures(degree=2)),('scaler', StandardScaler()),('regressor', LinearRegression())])\n",
    "    pipePolinomial.fit(X_train, y_train)\n",
    "\n",
    "    #Se obtiene el rendimiento en entrenamiento y en test (MSE)\n",
    "    prTrain = pipePolinomial.predict(X_train)\n",
    "    errorTrain = mean_squared_error(y_train, prTrain)\n",
    "    prTest = pipePolinomial.predict(X_test)\n",
    "    errorTest = mean_squared_error(y_test, prTest)\n",
    "    print(f\"Resutados para la eliminacion de {par[0]} y {par[1]}\")\n",
    "    print('El error en entrenamiento es {:.4f} y en test es {:.4f}'.format(errorTrain, errorTest))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Eliminación* *de* *las* *tres* *variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset1, dataset_output1, test_size=0.2, random_state=12)\n",
    "    \n",
    "X_train = X_train.drop(['open'] ,axis=1)\n",
    "X_train = X_train.drop(['low'],axis=1)\n",
    "X_train = X_train.drop(['high'] ,axis=1)\n",
    "\n",
    "X_test = X_test.drop(['open'] ,axis=1)\n",
    "X_test = X_test.drop(['low'],axis=1)\n",
    "X_test = X_test.drop(['high'] ,axis=1)\n",
    "\n",
    "pipePolinomial = Pipeline([('polynomial', PolynomialFeatures(degree=2)),('scaler', StandardScaler()),('regressor', LinearRegression())])\n",
    "pipePolinomial.fit(X_train, y_train)\n",
    "\n",
    "#Se obtiene el rendimiento en entrenamiento y en test (MSE)\n",
    "prTrain = pipePolinomial.predict(X_train)\n",
    "errorTrain = mean_squared_error(y_train, prTrain)\n",
    "prTest = pipePolinomial.predict(X_test)\n",
    "errorTest = mean_squared_error(y_test, prTest)\n",
    "print(\"Resultado para la eliminacion de high, low, y open\")\n",
    "print('El error en entrenamiento es {:.4f} y en test es {:.4f}'.format(errorTrain, errorTest))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras los experimentos realizados, se puede observar que se obtiene un peor resultado al tratar de eliminar cualquiera de las tres variables que cuentan con una mayor correlación -casi perfecta- .\n",
    "\n",
    "Por otro lado, en caso de eliminar una variable,la que menos impacto tiene en el rendimiento del modelo es 'open', y la que más influye es 'high'. \n",
    "\n",
    "En caso de eliminar dos variables,el mayor impacto negativo en el rendimiento vendrá dado por la eliminación de conjunta de 'low' y 'high'. Cosa que es natural, pues representa el valor más bajo y más alto de las acciones de una empresa en un día.\n",
    "\n",
    "La eliminación de 'high', 'low' y 'open' provoca un desenlace nefasto y esperado, pues predecir el valor al final del día de las acciones a partir del volumen de acciones negociadas es sumamente complicado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparativas de métodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso de regresión polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error en entrenamiento es 0.4663 y en test es 0.4903\n"
     ]
    }
   ],
   "source": [
    "regresion_polinomial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the packages \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras\n",
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de datos\n",
    "todataframe = pd.read_csv('all_stocks_5yr.csv', delimiter=',')\n",
    "#se eliminan las filas que tienen valores nulos de cara a realizar posteriores estudios\n",
    "#HABRIA QUE TENERLO IMPUTADO CON VALORES PERDIDOS!!\n",
    "todataframe.dropna(axis=0, inplace = True) \n",
    "#atSalida = 'close'\n",
    "#atEntrada = ['date','open','high','low','volume']\n",
    "#dataset_output = dataset[atSalida].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La comparativa del método se hará con la empresa AAl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "todataframe = todataframe[todataframe['Name']=='AAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>14.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2013-02-11</td>\n",
       "      <td>14.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2013-02-12</td>\n",
       "      <td>14.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>14.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>13.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1254</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>53.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>52.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1256</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>49.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1257</td>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1258</td>\n",
       "      <td>2018-02-07</td>\n",
       "      <td>51.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1259 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  close\n",
       "0     2013-02-08  14.75\n",
       "1     2013-02-11  14.46\n",
       "2     2013-02-12  14.27\n",
       "3     2013-02-13  14.66\n",
       "4     2013-02-14  13.99\n",
       "...          ...    ...\n",
       "1254  2018-02-01  53.88\n",
       "1255  2018-02-02   52.1\n",
       "1256  2018-02-05  49.76\n",
       "1257  2018-02-06  51.18\n",
       "1258  2018-02-07   51.4\n",
       "\n",
       "[1259 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creacion del dataframe\n",
    "seriesdata = todataframe.sort_index(ascending=True, axis=0)\n",
    "new_seriesdata = pd.DataFrame(index=range(0,len(todataframe)),columns=['date','close'])\n",
    "length_of_data=len(seriesdata)\n",
    "for i in range(0,length_of_data):\n",
    "    new_seriesdata['date'][i] = seriesdata['date'][i]\n",
    "    new_seriesdata['close'][i] = seriesdata['close'][i]\n",
    "new_seriesdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colocar los indices de nuevo\n",
    "new_seriesdata.index = new_seriesdata.date\n",
    "new_seriesdata.drop('date', axis=1, inplace=True)\n",
    "#Crear un conjunto de entrenamiento y de test (70% para train, 30% para test)\n",
    "numero_ejemplos_entrenamiento = int(len(new_seriesdata)*0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and test sets this comprises the entire data’s present in the dataset\n",
    "myseriesdataset = new_seriesdata.values\n",
    "totrain = myseriesdataset[0:numero_ejemplos_entrenamiento,:]\n",
    "tovalid = myseriesdataset[numero_ejemplos_entrenamiento:,:]\n",
    "\n",
    "#converting dataset into x_train and y_train\n",
    "scalerdata = MinMaxScaler(feature_range=(0, 1))\n",
    "scale_data = scalerdata.fit_transform(myseriesdataset)\n",
    "x_totrain, y_totrain = [], []\n",
    "length_of_totrain=len(totrain)\n",
    "for i in range(60,length_of_totrain):\n",
    "    x_totrain.append(scale_data[i-60:i,0])\n",
    "    y_totrain.append(scale_data[i,0])\n",
    "x_totrain, y_totrain = np.array(x_totrain), np.array(y_totrain)\n",
    "x_totrain = np.reshape(x_totrain, (x_totrain.shape[0],x_totrain.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "821/821 - 25s - loss: 0.1847\n",
      "Epoch 2/3\n",
      "821/821 - 23s - loss: 0.1254\n",
      "Epoch 3/3\n",
      "821/821 - 23s - loss: 0.0740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d9a1db44c8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM neural network\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=50, return_sequences=True, input_shape=(x_totrain.shape[1],1)))\n",
    "lstm_model.add(LSTM(units=50))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='adadelta')\n",
    "lstm_model.fit(x_totrain, y_totrain, epochs=3, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecir\n",
    "myinputs = new_seriesdata[len(new_seriesdata) - (len(tovalid)+1) - 60:].values\n",
    "myinputs = myinputs.reshape(-1,1)\n",
    "myinputs  = scalerdata.transform(myinputs)\n",
    "tostore_test_result = []\n",
    "for i in range(60,myinputs.shape[0]):\n",
    "    tostore_test_result.append(myinputs[i-60:i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tostore_test_result = np.array(tostore_test_result)\n",
    "tostore_test_result = np.reshape(tostore_test_result,(tostore_test_result.shape[0],tostore_test_result.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3102483 ],\n",
       "       [0.31000102],\n",
       "       [0.30972663],\n",
       "       [0.30935878],\n",
       "       [0.30911124],\n",
       "       [0.3089827 ],\n",
       "       [0.30917886],\n",
       "       [0.3098604 ],\n",
       "       [0.31088534],\n",
       "       [0.31214866],\n",
       "       [0.3135089 ],\n",
       "       [0.31477386],\n",
       "       [0.31600252],\n",
       "       [0.3172405 ],\n",
       "       [0.31847778],\n",
       "       [0.31956786],\n",
       "       [0.3204845 ],\n",
       "       [0.32145154],\n",
       "       [0.32228884],\n",
       "       [0.32296646],\n",
       "       [0.3235718 ],\n",
       "       [0.3242212 ],\n",
       "       [0.3253274 ],\n",
       "       [0.32696044],\n",
       "       [0.32877177],\n",
       "       [0.330594  ],\n",
       "       [0.33213365],\n",
       "       [0.33314425],\n",
       "       [0.33368975],\n",
       "       [0.33365527],\n",
       "       [0.33314317],\n",
       "       [0.3322216 ],\n",
       "       [0.3309017 ],\n",
       "       [0.3294472 ],\n",
       "       [0.3280333 ],\n",
       "       [0.32656154],\n",
       "       [0.32532477],\n",
       "       [0.324302  ],\n",
       "       [0.32346058],\n",
       "       [0.3229642 ],\n",
       "       [0.32302636],\n",
       "       [0.3235294 ],\n",
       "       [0.32440025],\n",
       "       [0.32559475],\n",
       "       [0.32689968],\n",
       "       [0.32849735],\n",
       "       [0.33014894],\n",
       "       [0.33157325],\n",
       "       [0.3331809 ],\n",
       "       [0.33481705],\n",
       "       [0.33639336],\n",
       "       [0.33805427],\n",
       "       [0.34005845],\n",
       "       [0.34231147],\n",
       "       [0.34453222],\n",
       "       [0.34661847],\n",
       "       [0.34850666],\n",
       "       [0.3501023 ],\n",
       "       [0.35155332],\n",
       "       [0.35291764],\n",
       "       [0.3543139 ],\n",
       "       [0.3555429 ],\n",
       "       [0.3565617 ],\n",
       "       [0.3573162 ],\n",
       "       [0.3579215 ],\n",
       "       [0.3588117 ],\n",
       "       [0.3600195 ],\n",
       "       [0.36160737],\n",
       "       [0.36344334],\n",
       "       [0.3655473 ],\n",
       "       [0.36792025],\n",
       "       [0.37074378],\n",
       "       [0.37381786],\n",
       "       [0.37727427],\n",
       "       [0.38107064],\n",
       "       [0.38499516],\n",
       "       [0.38889214],\n",
       "       [0.3927086 ],\n",
       "       [0.39646816],\n",
       "       [0.40003788],\n",
       "       [0.403433  ],\n",
       "       [0.4065315 ],\n",
       "       [0.40916133],\n",
       "       [0.4114082 ],\n",
       "       [0.4132499 ],\n",
       "       [0.4148737 ],\n",
       "       [0.41678753],\n",
       "       [0.41886565],\n",
       "       [0.42131087],\n",
       "       [0.42363173],\n",
       "       [0.4257415 ],\n",
       "       [0.42765653],\n",
       "       [0.42945376],\n",
       "       [0.43100095],\n",
       "       [0.43247783],\n",
       "       [0.43404034],\n",
       "       [0.43571317],\n",
       "       [0.43728527],\n",
       "       [0.43865737],\n",
       "       [0.4398553 ],\n",
       "       [0.44071066],\n",
       "       [0.44115588],\n",
       "       [0.44114885],\n",
       "       [0.44069952],\n",
       "       [0.439997  ],\n",
       "       [0.43898672],\n",
       "       [0.43780616],\n",
       "       [0.4367279 ],\n",
       "       [0.43609023],\n",
       "       [0.43590948],\n",
       "       [0.43598   ],\n",
       "       [0.43611544],\n",
       "       [0.436084  ],\n",
       "       [0.4360552 ],\n",
       "       [0.43599254],\n",
       "       [0.4360372 ],\n",
       "       [0.4359843 ],\n",
       "       [0.43592846],\n",
       "       [0.43597403],\n",
       "       [0.43645176],\n",
       "       [0.4367989 ],\n",
       "       [0.43648952],\n",
       "       [0.4354479 ],\n",
       "       [0.43380484],\n",
       "       [0.4317399 ],\n",
       "       [0.42946044],\n",
       "       [0.42733032],\n",
       "       [0.42540607],\n",
       "       [0.42366862],\n",
       "       [0.4223737 ],\n",
       "       [0.42153585],\n",
       "       [0.42126954],\n",
       "       [0.42130712],\n",
       "       [0.421724  ],\n",
       "       [0.42233342],\n",
       "       [0.42302352],\n",
       "       [0.42372257],\n",
       "       [0.424298  ],\n",
       "       [0.42469096],\n",
       "       [0.42495945],\n",
       "       [0.42514443],\n",
       "       [0.4252742 ],\n",
       "       [0.42551294],\n",
       "       [0.42558497],\n",
       "       [0.42569008],\n",
       "       [0.42555916],\n",
       "       [0.4250857 ],\n",
       "       [0.42435226],\n",
       "       [0.4231223 ],\n",
       "       [0.42160007],\n",
       "       [0.4196057 ],\n",
       "       [0.41699737],\n",
       "       [0.41413435],\n",
       "       [0.41121125],\n",
       "       [0.40824616],\n",
       "       [0.4053137 ],\n",
       "       [0.40220767],\n",
       "       [0.3989836 ],\n",
       "       [0.39598563],\n",
       "       [0.39335722],\n",
       "       [0.39109808],\n",
       "       [0.38936654],\n",
       "       [0.3879714 ],\n",
       "       [0.38696137],\n",
       "       [0.3862313 ],\n",
       "       [0.3857465 ],\n",
       "       [0.38511676],\n",
       "       [0.38440865],\n",
       "       [0.3837604 ],\n",
       "       [0.3832116 ],\n",
       "       [0.38286927],\n",
       "       [0.38306594],\n",
       "       [0.3837407 ],\n",
       "       [0.38462493],\n",
       "       [0.3857975 ],\n",
       "       [0.38709506],\n",
       "       [0.388553  ],\n",
       "       [0.39028755],\n",
       "       [0.3922023 ],\n",
       "       [0.39446697],\n",
       "       [0.3970243 ],\n",
       "       [0.3996929 ],\n",
       "       [0.40184924],\n",
       "       [0.40314925],\n",
       "       [0.40358177],\n",
       "       [0.4037076 ],\n",
       "       [0.40370536],\n",
       "       [0.40362483],\n",
       "       [0.40362543],\n",
       "       [0.40381348],\n",
       "       [0.40461427],\n",
       "       [0.40600288],\n",
       "       [0.40764916],\n",
       "       [0.40925166],\n",
       "       [0.4109079 ],\n",
       "       [0.41264617],\n",
       "       [0.41399202],\n",
       "       [0.4148477 ],\n",
       "       [0.41541246],\n",
       "       [0.41598025],\n",
       "       [0.4166845 ],\n",
       "       [0.4175252 ],\n",
       "       [0.4187115 ],\n",
       "       [0.4203339 ],\n",
       "       [0.4221177 ],\n",
       "       [0.42402178],\n",
       "       [0.42610747],\n",
       "       [0.4283948 ],\n",
       "       [0.43083546],\n",
       "       [0.4333324 ],\n",
       "       [0.43601948],\n",
       "       [0.43894163],\n",
       "       [0.4417642 ],\n",
       "       [0.4442162 ],\n",
       "       [0.44626963],\n",
       "       [0.44795355],\n",
       "       [0.4492607 ],\n",
       "       [0.4501786 ],\n",
       "       [0.450939  ],\n",
       "       [0.4512954 ],\n",
       "       [0.45134035],\n",
       "       [0.45128053],\n",
       "       [0.4511132 ],\n",
       "       [0.45088923],\n",
       "       [0.4505852 ],\n",
       "       [0.4503665 ],\n",
       "       [0.45033106],\n",
       "       [0.45059392],\n",
       "       [0.45112428],\n",
       "       [0.45201233],\n",
       "       [0.45333833],\n",
       "       [0.4551816 ],\n",
       "       [0.4573388 ],\n",
       "       [0.45944655],\n",
       "       [0.46180347],\n",
       "       [0.46439517],\n",
       "       [0.4671672 ],\n",
       "       [0.46995157],\n",
       "       [0.47251254],\n",
       "       [0.47469148],\n",
       "       [0.47644168],\n",
       "       [0.47772953],\n",
       "       [0.47849956],\n",
       "       [0.47870675],\n",
       "       [0.478531  ],\n",
       "       [0.47791383],\n",
       "       [0.47703025],\n",
       "       [0.47599536],\n",
       "       [0.4749998 ],\n",
       "       [0.47398558],\n",
       "       [0.4729774 ],\n",
       "       [0.47205496],\n",
       "       [0.47119486],\n",
       "       [0.47028318],\n",
       "       [0.46921027],\n",
       "       [0.4678468 ],\n",
       "       [0.46621662],\n",
       "       [0.46450907],\n",
       "       [0.46276006],\n",
       "       [0.46099564],\n",
       "       [0.4587931 ],\n",
       "       [0.4561024 ],\n",
       "       [0.4531201 ],\n",
       "       [0.44994912],\n",
       "       [0.44641823],\n",
       "       [0.4424275 ],\n",
       "       [0.43863246],\n",
       "       [0.43513846],\n",
       "       [0.43188128],\n",
       "       [0.428822  ],\n",
       "       [0.42606762],\n",
       "       [0.42375162],\n",
       "       [0.42155948],\n",
       "       [0.41953132],\n",
       "       [0.41759017],\n",
       "       [0.41568902],\n",
       "       [0.41433063],\n",
       "       [0.4136092 ],\n",
       "       [0.41355664],\n",
       "       [0.41386667],\n",
       "       [0.4143528 ],\n",
       "       [0.41479135],\n",
       "       [0.41494396],\n",
       "       [0.41504145],\n",
       "       [0.41532186],\n",
       "       [0.41593704],\n",
       "       [0.41695887],\n",
       "       [0.4182648 ],\n",
       "       [0.41958973],\n",
       "       [0.42098022],\n",
       "       [0.42240688],\n",
       "       [0.42386836],\n",
       "       [0.42588562],\n",
       "       [0.42844787],\n",
       "       [0.43132842],\n",
       "       [0.43449724],\n",
       "       [0.43768796],\n",
       "       [0.44123244],\n",
       "       [0.4450281 ],\n",
       "       [0.4488611 ],\n",
       "       [0.45260936],\n",
       "       [0.4561073 ],\n",
       "       [0.45925754],\n",
       "       [0.46204194],\n",
       "       [0.4643792 ],\n",
       "       [0.46637264],\n",
       "       [0.46790567],\n",
       "       [0.46919   ],\n",
       "       [0.4701388 ],\n",
       "       [0.4703129 ],\n",
       "       [0.4695717 ],\n",
       "       [0.46804836],\n",
       "       [0.46588707],\n",
       "       [0.46349642],\n",
       "       [0.4609696 ],\n",
       "       [0.45841762],\n",
       "       [0.45594013],\n",
       "       [0.45338437],\n",
       "       [0.4507706 ],\n",
       "       [0.4480559 ],\n",
       "       [0.44531912],\n",
       "       [0.44262502],\n",
       "       [0.44004717],\n",
       "       [0.43791273],\n",
       "       [0.43633637],\n",
       "       [0.43518853],\n",
       "       [0.43443584],\n",
       "       [0.43420032],\n",
       "       [0.4344011 ],\n",
       "       [0.43489698],\n",
       "       [0.43555763],\n",
       "       [0.4364668 ],\n",
       "       [0.43757614],\n",
       "       [0.43904752],\n",
       "       [0.44051558],\n",
       "       [0.44204274],\n",
       "       [0.4435225 ],\n",
       "       [0.44492814],\n",
       "       [0.44649205],\n",
       "       [0.44822592],\n",
       "       [0.45010355],\n",
       "       [0.45189124],\n",
       "       [0.4534428 ],\n",
       "       [0.4547738 ],\n",
       "       [0.45607227],\n",
       "       [0.45744193],\n",
       "       [0.45884585],\n",
       "       [0.46032834],\n",
       "       [0.4620198 ],\n",
       "       [0.463832  ],\n",
       "       [0.4657231 ],\n",
       "       [0.46754867],\n",
       "       [0.4692594 ],\n",
       "       [0.470758  ],\n",
       "       [0.47221133],\n",
       "       [0.47352466],\n",
       "       [0.47472656],\n",
       "       [0.47582787],\n",
       "       [0.47672954],\n",
       "       [0.47742122],\n",
       "       [0.47824988],\n",
       "       [0.47973797],\n",
       "       [0.48219863],\n",
       "       [0.48535076],\n",
       "       [0.4889283 ],\n",
       "       [0.4927495 ],\n",
       "       [0.496595  ],\n",
       "       [0.5003305 ],\n",
       "       [0.5039064 ],\n",
       "       [0.5066774 ],\n",
       "       [0.50825554],\n",
       "       [0.5087931 ],\n",
       "       [0.50847656],\n",
       "       [0.50750667],\n",
       "       [0.5063944 ],\n",
       "       [0.50524384],\n",
       "       [0.5037694 ],\n",
       "       [0.501571  ],\n",
       "       [0.49902022]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myclosing_priceresult = lstm_model.predict(tostore_test_result)\n",
    "myclosing_priceresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclosing_priceresult = scalerdata.inverse_transform(myclosing_priceresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myclosing_priceresult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37machlearn] *",
   "language": "python",
   "name": "conda-env-py37machlearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduccion de variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from category_encoders) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from category_encoders) (1.16.4)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: pandas>=0.21.1 in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from category_encoders) (0.25.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from category_encoders) (0.12.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from category_encoders) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (0.13.2)\n",
      "Requirement already satisfied: six in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.12.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2019.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\daviz.desktop-829tkaf\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "from test_helper import Test\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    " \n",
    "#%config InlineBackend.figure_formats = {'pdf',}\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics, neighbors, tree, preprocessing\n",
    "from scipy import stats\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import neighbors, model_selection, tree\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "\n",
    "# Se importa la librería de selección de variables\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('all_stocks_5yr.csv', delimiter=',')\n",
    "#se eliminan las filas que tienen valores nulos de cara a realizar posteriores estudios\n",
    "#HABRIA QUE TENERLO IMPUTADO CON VALORES PERDIDOS!!\n",
    "dataset.dropna(axis=0, inplace = True)\n",
    "\n",
    "atSalida = 'close'\n",
    "atEntrada = ['open','high','low','volume']\n",
    "\n",
    "dataset = dataset[dataset['Name']=='AAL']\n",
    "dataset_output = dataset[atSalida].copy()\n",
    "dataset.drop(atSalida, axis=1, inplace=True)\n",
    "dataset = dataset[atEntrada].copy()\n",
    "\n",
    "\n",
    "# SUSTITUIR POR LO BUENO\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, dataset_output, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccion de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos diferentes tecnicas de seleccion de variables, compararemos sus errores y eligiremos la mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Correlación mutua: función [*f_regression*](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression)\n",
    "* Información mutua: función [*mutual_info_regression*](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)\n",
    "\n",
    "\n",
    "Una vez conocida la calidad de cada variable se deben escoger las mejores. Para ello vimos en las clases teóricas que había varias opciones. La librería Scikit-learn ofrece dos de estas técnicas en forma de clases (con sus campos y sus métodos):\n",
    "* Elegir las k mejores con la clase [*SelectKBest*](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)\n",
    "* Elegir las variables en base al percentil (el % de las variables) con la clase [*SelectPercentile*](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Correlación mutua: función [*f_regression*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la Pipeline con las fases deseadas de Correlación mutua\n",
    "pipe_f_regression = Pipeline([\n",
    "    ('correlacion_mutua', feature_selection.SelectPercentile(feature_selection.f_regression)),\n",
    "    ('polynomial', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())])\n",
    "# Se crea el grid de híper-parámetros\n",
    "parameters_f_regression = {'correlacion_mutua__percentile': [10, 20, 30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Información mutua: función [*mutual_info_regression*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la Pipeline con las fases deseadas de Informacion mutua\n",
    "pipe_mutual_info_regression = Pipeline([\n",
    "    ('polynomial', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('informacion_mutua', feature_selection.SelectPercentile(feature_selection.mutual_info_regression,percentile=10)),\n",
    "    ('regressor', LinearRegression())])\n",
    "# Se crea el grid de híper-parámetros\n",
    "parameters_mutual_info_regression = {'informacion_mutua__percentile': [10, 20, 30]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9977987477718774\n",
      "{'correlacion_mutua__percentile': 10}\n",
      "0.9977987477718774 {'correlacion_mutua__percentile': 10}\n",
      "Resultado en entrenamiento: 99.81172914050642%\n",
      "Resultado en test: 99.71610456822759%\n"
     ]
    }
   ],
   "source": [
    "# Selecciona la pipe a utilizar\n",
    "pipe = pipe_f_regression\n",
    "parameters = parameters_f_regression\n",
    "\n",
    "# Se llama al constructor de GridSearchCV para que genere todas las combinaciones de los híper-parámetros definidos anteriormente\n",
    "gridSearch_pipe = model_selection.GridSearchCV(pipe,parameters,cv=3,return_train_score=True)\n",
    "# Se realiza el aprendizaje de todos los clasificadores considerados (todas las configuraciones)\n",
    "gridSearch_pipe = gridSearch_pipe.fit(X_train,y_train.values.ravel())\n",
    "# Se muestra la mejor configuración junto con su rendimiento\n",
    "print(gridSearch_pipe.best_score_)\n",
    "print(gridSearch_pipe.best_params_)\n",
    "\n",
    "\n",
    "# Almacenamos el DataFrame con los resultados\n",
    "diccionarioResultados = gridSearch_pipe.cv_results_\n",
    "\n",
    "# Se imprime el mejor porcentaje de acierto y los resultados de todas las configuraciones\n",
    "print(gridSearch_pipe.best_score_,gridSearch_pipe.best_params_)\n",
    "\n",
    "# Se obtiene el rendimiento en entrenamiento y test por la mejor configuración\n",
    "accTrain = gridSearch_pipe.score(X_train,y_train.values.ravel())*100\n",
    "accTest = gridSearch_pipe.score(X_test,y_test.values.ravel())*100\n",
    "print('Resultado en entrenamiento: {}%'.format(accTrain))\n",
    "print('Resultado en test: {}%'.format(accTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=0.1,\n",
       "                     random_state=None, svd_solver='full', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('polynomial',\n",
       "                 PolynomialFeatures(degree=2, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('regressor',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mutual_info_regression = Pipeline([\n",
    "    ('polynomial', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('informacion_mutua', feature_selection.SelectPercentile(feature_selection.mutual_info_regression,percentile=10)),\n",
    "    ('regressor', LinearRegression())])\n",
    "gridSearch_pipe = pipe_mutual_info_regression.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11193190724536053\n",
      "{'pca__n_components': 0.1}\n",
      "-0.11193190724536053 {'pca__n_components': 0.1}\n",
      "Resultado en entrenamiento: 0.11252279355632444%\n",
      "Resultado en test: -0.43812629193509245%\n"
     ]
    }
   ],
   "source": [
    "# Se crea la Pipeline con las fases deseadas para el pipe PCA\n",
    "pipe_pca = Pipeline([('pca', PCA(svd_solver='full')),\n",
    "                 ('polynomial', PolynomialFeatures(degree=2)),\n",
    "                 ('scaler', StandardScaler()),\n",
    "                 ('regressor', LinearRegression())])\n",
    "# Se crea el grid de híper-parámetros\n",
    "parameters_pca = {'pca__n_components': [0.10, 0.30, 0.50, 0.70, 0.90]}\n",
    "# Se llama al constructor de GridSearchCV para que genere todas las combinaciones de los híper-parámetros definidos anteriormente\n",
    "gridSearch_pipe = model_selection.GridSearchCV(pipe_pca,parameters_pca,cv=10,n_jobs=-1,return_train_score=True)\n",
    "# Se realiza el aprendizaje de todos los clasificadores considerados (todas las configuraciones)\n",
    "gridSearch_pipe =  gridSearch_pipe.fit(X_train,y_train)\n",
    "# Se muestra la mejor configuración junto con su rendimiento\n",
    "print(gridSearch_pipe.best_score_)\n",
    "print(gridSearch_pipe.best_params_)\n",
    "\n",
    "\n",
    "# Almacenamos el DataFrame con los resultados\n",
    "diccionarioResultados = gridSearch_pipe.cv_results_\n",
    "\n",
    "# Se imprime el mejor porcentaje de acierto y los resultados de todas las configuraciones\n",
    "print(gridSearch_pipe.best_score_,gridSearch_pipe.best_params_)\n",
    "\n",
    "# Se obtiene el rendimiento en entrenamiento y test por la mejor configuración\n",
    "accTrain = gridSearch_pipe.score(X_train,y_train.values.ravel())*100\n",
    "accTest = gridSearch_pipe.score(X_test,y_test.values.ravel())*100\n",
    "print('Resultado en entrenamiento: {}%'.format(accTrain))\n",
    "print('Resultado en test: {}%'.format(accTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccióndeinstancias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos Wrapers ya que miran la precision con Leave One Out y no tiene en cuanta las clases, las cuales no nos importan ennuestro modelo de regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMHC(regresor, X, y, s=0.1, iteraciones=1000):\n",
    "    \"\"\"\n",
    "     Algoritmo RMHC (Random Mutation Hill Climbing) para la selección de instancias.\n",
    "      Se comienza con una selección aleatoria de s * nEjemplos instancias. Para cada iteración, se elige una instancia\n",
    "      seleccionada y una no seleccionada para ser intercambiadas. Si el intercambio mejora la precisión (leave-one-out) sobre train\n",
    "      se mantiene el cambio, sino se deshace\n",
    "    :param X: Matriz con los ejemplos de entrenamiento (se asume que los ejemplos están normalizados)\n",
    "    :param y: Vector con la salida de los ejemplos en X\n",
    "    :param s: Porcentaje de instancias a ser seleccionadas (s es un valor entre 0 y 1)\n",
    "    :param iteraciones: Número de iteraciones (intercambios) a probar\n",
    "    :return: Vector con la máscara de instancias seleccionadas \n",
    "            (La posición S[i]=True indica que la instancia i ha sido seleccionada y False lo contrario)\n",
    "    \"\"\"\n",
    "    print(X_train)\n",
    "    # Como usamos aleatorios en RMHC establecemos la semilla para que el test sea correcto y siempre obtengamos el mismo resultado\n",
    "    np.random.seed(12312)\n",
    "\n",
    "    # En este caso vamos a mantener dos vectores de enteros con los índices de las instancias seleccionadas y no seleccionadas\n",
    "    # Establecemos el número de instancias a seleccionar (es fijo)\n",
    "    nSel = int(X.shape[0]*s)\n",
    "    \n",
    "    # Obtenemos una permutación aleatoria de todos los índices disponibles \n",
    "    # (un array de 0 a X.shape[0] con valores ordenados aleatoriamente): utilizad permutation de Numpy\n",
    "    permute = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    # Cogemos como seleccionadas las instancias correspondientes a los primeros nSel índices  y no seleccionadas el resto#     \n",
    "    seleccionadas = permute[:nSel]\n",
    "    noSeleccionadas = permute[nSel:]\n",
    "    print(seleccionadas)\n",
    "    # Calculamos la preicisión con la selección inicial\n",
    "    # Entrenamos regresor con las instancias seleccionadas\n",
    "    regresor.fit(X[seleccionadas], y[seleccionadas])\n",
    "    \n",
    "    # Obtenemos las salidas con leaveOneOut (para no tener en cuenta las instancias seleccionadas como vecinos de sí mismas)\n",
    "    salidas = regresor.predict(X)\n",
    "        \n",
    "    # Calculamos la precisión\n",
    "    acc = metrics.accuracy_score(salidas, y)\n",
    "\n",
    "    # Comenzamos las iteraciones:\n",
    "    #   En cada una, intercambiamos una instancia seleccionada con una que no lo está y comprobamos si mejora la precisión\n",
    "    for i in range(0, iteraciones):\n",
    "        # Seleccionamos un índice de manera aleatoria que nos indica la instancia a eliminar de las seleccionadas: randint de Numpy\n",
    "        quitar = np.random.randint(seleccionadas.size)\n",
    "        \n",
    "        # Seleccionamos un índice de manera aleatoria que nos indica la instancia a añadir a las seleccionadas: randint de Numpy\n",
    "        poner = np.random.randint(noSeleccionadas.size)\n",
    "        \n",
    "        # Guardamos el ejemplo eliminado\n",
    "        aux =  seleccionadas[quitar]\n",
    "        \n",
    "        # Añadimos la nueva instancia seleccionada sustituyendo la que se elimina: intercambio en las listas de seleccionados y no seleccionados\n",
    "        seleccionadas[quitar] = noSeleccionadas[poner]\n",
    "        \n",
    "        # Entrenamos de nuevo regresor\n",
    "        regresor.fit(X[seleccionadas], y[seleccionadas])\n",
    "\n",
    "        # Calculamos la precisión de la solución actual\n",
    "        # Obtenemos las salidas con leaveOneOut (para no tener en cuenta las instancias seleccionadas como vecinos de sí mismas)\n",
    "        salidas = regresor.predict(X)\n",
    "        \n",
    "        # Calculamos la precisión\n",
    "        accNew = metrics.accuracy_score(salidas, y)\n",
    "\n",
    "        # Si la precisión actual es peor que la anterior, devolvemos la instancia eliminada a las seleccionadas\n",
    "        # Sino si la precisión de la solución actual es mejor o igual que la anterior, guardamos la precisión y\n",
    "        #  añadimos la instancia eliminada a las no seleccionadas\n",
    "        if accNew >= acc:\n",
    "            acc = accNew\n",
    "            noSeleccionadas[poner] = aux\n",
    "        else:\n",
    "            seleccionadas[quitar] = aux\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"precision en iteracion {}: {}\".format(i, acc))\n",
    "\n",
    "    # A partir de las seleccionadas creamos la máscara de instancias seleccionadas \n",
    "    # donde True en la posición i indica que la instancia i es seleccionada\n",
    "    S = np.zeros(len(y), bool)\n",
    "    S[seleccionadas] = True\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.7860000e+01, 1.8000000e+01, 1.7750000e+01, 3.8391000e+06],\n",
       "       [3.5780000e+01, 3.6080000e+01, 3.5330000e+01, 7.9562710e+06],\n",
       "       [1.8790000e+01, 1.9280000e+01, 1.8680000e+01, 5.6109000e+06],\n",
       "       ...,\n",
       "       [1.5990000e+01, 1.6000000e+01, 1.5500000e+01, 9.2271000e+06],\n",
       "       [3.7270000e+01, 3.7630000e+01, 3.6680000e+01, 3.8630640e+06],\n",
       "       [3.9600000e+01, 4.0400000e+01, 3.9290000e+01, 1.3355207e+07]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('polynomial', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('informacion_mutua', feature_selection.SelectPercentile(feature_selection.mutual_info_regression,percentile=10)),\n",
    "    ('regressor', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2,include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train.values)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaler = scaler.fit_transform(X_train)\n",
    "\n",
    "sp = SelectPercentile(feature_selection.mutual_info_regression, percentile=10)\n",
    "X_train_poly_scaler_sp = sp.fit_transform(X_train_poly_scaler, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87705818, -1.68294735],\n",
       "       [-0.24641643, -0.42231598],\n",
       "       [-1.76161451, -1.62142416],\n",
       "       ...,\n",
       "       [-2.0574389 , -1.77061955],\n",
       "       [-0.10662137, -0.27501315],\n",
       "       [ 0.14320593,  0.00365952]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_poly_scaler_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      open   high    low    volume\n",
      "108  17.86  18.00  17.75   3839100\n",
      "866  35.78  36.08  35.33   7956271\n",
      "157  18.79  19.28  18.68   5610900\n",
      "9    13.57  13.60  13.21   6071400\n",
      "134  15.80  15.99  15.75   4234300\n",
      "..     ...    ...    ...       ...\n",
      "418  34.75  35.02  34.06  10032531\n",
      "473  50.66  51.69  50.35   5961395\n",
      "49   15.99  16.00  15.50   9227100\n",
      "278  37.27  37.63  36.68   3863064\n",
      "432  39.60  40.40  39.29  13355207\n",
      "\n",
      "[1007 rows x 4 columns]\n",
      "[134 791 583 167 200 670 373 129 674  80 315 788 613 894 343 644 944 236\n",
      " 303 415  19 400 734 862 166  30 619 906 968 871 387 604 863 512 810 124\n",
      " 188 897 492 139 123 861 507 695 848 623 130 111 694 718  64 622 156 931\n",
      " 859 237 185 802 221 955 210 852 666 216  50 570 202 685 133 652 494 972\n",
      " 820 815 713 823 697 280 501 390 625 910  63 768 131 830 227 241 720 811\n",
      " 665 786 466 984  31 224 678 598 945 748]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daviz.DESKTOP-829TKAF\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pandas\\core\\series.py:1143: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f79ea6673ef5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRMHC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_poly_scaler_sp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-e38e62aab696>\u001b[0m in \u001b[0;36mRMHC\u001b[1;34m(regresor, X, y, s, iteraciones)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Calculamos la preicisión con la selección inicial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Entrenamos regresor con las instancias seleccionadas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mregresor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseleccionadas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseleccionadas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Obtenemos las salidas con leaveOneOut (para no tener en cuenta las instancias seleccionadas como vecinos de sí mismas)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 492\u001b[1;33m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n\u001b[1;32m--> 758\u001b[1;33m                         dtype=None)\n\u001b[0m\u001b[0;32m    759\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 578\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     (type_err,\n\u001b[1;32m---> 60\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "S = RMHC(LinearRegression(), X_train_poly_scaler_sp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape[0])\n",
    "print(S.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROPHET FACEBOOK IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset solo tiene las variables 'date' y 'close' (o la ue se quiera predecir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset_valor_anterior.copy()\n",
    "dataset = dataset[dataset.Name==\"AAPL\"]\n",
    "atEntrada = ['date','close']\n",
    "\n",
    "dataset = dataset[atEntrada].copy()\n",
    "df = dataset.rename(columns = {'date':'ds','close': 'y'}, inplace = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet()\n",
    "m.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=365)\n",
    "# print(future.tail())\n",
    "forecast = m.predict(future)\n",
    "forecast.tail()\n",
    "# forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "# fig1 = m.plot(forecast)\n",
    "# m.plot_components(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_components(forecast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
